{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98ce0e6e-2f02-4693-b2e6-0e0c60fbeb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d46aaac7-8aab-4fbd-bbb1-41f6d4f13f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"flickr30k\"\n",
    "\n",
    "\n",
    "paths = {\"flickr30k\": \"/raid/datasets/f30k\",\n",
    "         \"mscoco\": \"/raid/datasets/coco\"}\n",
    "path = paths[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a721d39-c50b-4d3b-9031-26e7a357a497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /srv/home/8wiehe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /srv/home/8wiehe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def filter_words(sent):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    #stop_words.remove(\"of\")\n",
    "    for w in [\"other\", \"scene\", \"notvisual\", \"one\", \"something\", \"foreground\", \"background\", \n",
    "              \"people\", \"many\", \"several\", \"bunch\", \"lot\", \"left\", \"right\", \"top\", \"bottom\", \"others\", \"another\"]:\n",
    "        stop_words.add(w)\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    filtered_sentence = [w.lower().translate(string.punctuation) for w in word_tokens]\n",
    "    filtered_sentence = [w for w in filtered_sentence if w not in stop_words and w.isalpha()]\n",
    "    return \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5013a07-9474-413c-a3de-1cf6c2100ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_key_names(inv_dict, k=10, verbose=False, filter_words=None):\n",
    "    sorted_keys = sorted(inv_dict, key=lambda x: len(set([l // 5 for l in inv_dict[x]])), reverse=True)\n",
    "    valid_label_names = []\n",
    "    count = 0\n",
    "    for key in sorted_keys:\n",
    "        if filter_words is not None:\n",
    "            if key in filter_words:\n",
    "                continue\n",
    "        if verbose:\n",
    "            print(key)\n",
    "            print(len(inv_dict[key]))\n",
    "        if len(key) > 2:\n",
    "            valid_label_names.append(key)\n",
    "        count += 1\n",
    "        if count == k:\n",
    "            break\n",
    "    return valid_label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aef966e-649d-4c66-bb73-abb6115d2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(sent_data):\n",
    "    labels = [[phrase[\"phrase\"] for phrase in sent_dict[\"phrases\"]] for sent_dict in sent_data]\n",
    "    labels += [[phrase[\"phrase_type\"] for phrase in sent_dict[\"phrases\"]] for sent_dict in sent_data]\n",
    "    flat_labels = set()\n",
    "    for label in labels:\n",
    "        if isinstance(label, list):\n",
    "            for l in label:\n",
    "                if isinstance(l, list):\n",
    "                    for lab in l:\n",
    "                        flat_labels.add(lab)\n",
    "                else:\n",
    "                    flat_labels.add(l)\n",
    "        else:\n",
    "            flat_labels.add(label)\n",
    "    flat_labels = list(flat_labels)\n",
    "    labels = sorted([element for element in set([filter_words(sent) for sent in flat_labels]) if len(element) > 2])\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa5e2cbf-9b9b-427c-ad0b-70ac2d7781fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# load img paths\n",
    "def is_img(img_name):\n",
    "    return img_name.endswith(\".jpg\") or img_name.endswith(\".jpeg\") or img_name.endswith(\".png\")\n",
    "\n",
    "\n",
    "def load_flickr(path, num_imgs=None):\n",
    "    img_folder = os.path.join(path, \"images\")\n",
    "    imgs = [i for i in os.listdir(img_folder) if is_img(i)]\n",
    "    imgs = sorted(imgs, key = lambda x: int(x.split(\".\")[0]))\n",
    "    img_ids = [int(x.split(\".\")[0]) for x in imgs]\n",
    "    img_paths = [os.path.join(img_folder, i) for i in imgs]\n",
    "\n",
    "    # load captions\n",
    "    captions = pd.read_csv(os.path.join(path, \"captions.txt\"), delimiter=\"|\")\n",
    "    captions[\"img_idx\"] = captions[\"image_name\"].apply(lambda x: int(x.split(\".\")[0]))\n",
    "    captions = captions.sort_values(\"img_idx\")\n",
    "    del captions[\"img_idx\"]\n",
    "    \n",
    "    if num_imgs is not None:\n",
    "        imgs = imgs[:num_imgs]\n",
    "        img_paths = imgs[:num_imgs]\n",
    "        img_ids = img_ids[:num_imgs]\n",
    "        captions = captions[:num_imgs * 5]\n",
    "    \n",
    "    # annotated bounding box labels:\n",
    "    # from https://github.com/BryanPlummer/flickr30k_entities\n",
    "    import sys\n",
    "    sys.path.append(\"/raid/datasets\")\n",
    "    from flickr30k_entities.flickr30k_entities_utils import get_annotations, get_sentence_data\n",
    "    from collections import defaultdict\n",
    "\n",
    "    labels = []\n",
    "    label_count = defaultdict(int)\n",
    "\n",
    "    # load relations between img bounding boxes and captions\n",
    "    for img_id in tqdm(img_ids):\n",
    "        #p = f\"/raid/datasets/flickr30k_entities/Annotations/{img_id}.xml\"\n",
    "        #annot = get_annotations(p)\n",
    "        p = f\"/raid/datasets/flickr30k_entities/Sentences/{img_id}.txt\"\n",
    "        sent_data = get_sentence_data(p)\n",
    "        img_labels = get_labels(sent_data)\n",
    "        # increase counts\n",
    "        for l in img_labels:\n",
    "            label_count[l] += 1\n",
    "        # store\n",
    "        labels.append(img_labels)\n",
    "\n",
    "    # get label names with highest count\n",
    "    top_k = 200\n",
    "    label_series = pd.Series(label_count.values(), index=label_count.keys()).sort_values(ascending=False).iloc[:top_k]\n",
    "    label_names = list(label_series.index)\n",
    "    # sort through all labels to only keep high count labels\n",
    "    labels = [[l for l in label_list if l in label_names] for label_list in labels]\n",
    "\n",
    "    figsize = (6, 50)\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(y=label_series.index, x=label_series)\n",
    "    plt.savefig(\"flickr30k/label_counts.png\")\n",
    "    plt.title(\"Label frequency Flickr30k\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # get training splits\n",
    "    test_ids = pd.read_csv(\"/raid/datasets/flickr30k_entities/test.txt\", sep=\"\\n\", header=None)\n",
    "    train_ids = pd.read_csv(\"/raid/datasets/flickr30k_entities/train.txt\", sep=\"\\n\", header=None)\n",
    "    val_ids = pd.read_csv(\"/raid/datasets/flickr30k_entities/val.txt\", sep=\"\\n\", header=None)\n",
    "    print(\"Num train samples: \", len(train_ids))\n",
    "    print(\"Num val samples: \", len(val_ids))\n",
    "    print(\"Num test samples: \", len(test_ids))\n",
    "    \n",
    "    #return img_paths, captions, labels, specific_inv_dict, broad_inv_dict, specific_dict, broad_dict, all_label_names\n",
    "    return img_paths, img_ids, captions, labels, train_ids, val_ids, test_ids, label_names\n",
    "\n",
    "\n",
    "def load_coco(path):\n",
    "    img_folder = os.path.join(path, \"images\")\n",
    "    imgs = [i for i in os.listdir(img_folder) if is_img(i)]\n",
    "    imgs = sorted(imgs, key = lambda x: int(x.split(\".\")[0].split(\"_\")[1]))\n",
    "    img_ids = [int(x.split(\".\")[0]) for x in imgs]\n",
    "    img_paths = [os.path.join(img_folder, i) for i in imgs]\n",
    "\n",
    "    # load captions\n",
    "    train_caps = json.load(open(\"annotations/captions_train2014.json\", \"r\"))\n",
    "    file_name = train_caps[\"images\"][0][\"file_name\"]\n",
    "    id_ = train_caps[\"images\"][0][\"id\"]\n",
    "    \n",
    "    # load captions\n",
    "    captions = pd.read_csv(os.path.join(path, \"captions.txt\"), delimiter=\"|\")\n",
    "    captions[\"img_idx\"] = captions[\"image_name\"].apply(lambda x: int(x.split(\".\")[0]))\n",
    "    captions = captions.sort_values(\"img_idx\")\n",
    "    del captions[\"img_idx\"]\n",
    "    return img_paths, captions\n",
    "\n",
    "\n",
    "    return img_paths, img_ids, captions, label_strs, train_ids, val_ids, test_ids, label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a8c3b-c10b-44dd-afec-8d14256b9b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993447e6bd9b4a299865c6660a84b84c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31783 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define dataset specifics\n",
    "if dataset == \"flickr30k\":\n",
    "    label_path = \"flickr30k/labels.npy\"\n",
    "    load_func = load_flickr\n",
    "elif dataset == \"coco\":\n",
    "    label_path = \"coco/labels.npy\"\n",
    "    load_func = load_coco\n",
    "\n",
    "# load or create\n",
    "if not os.path.exists(label_path):\n",
    "    out = load_func(path)\n",
    "    np.save(label_path, out)\n",
    "else:\n",
    "    out = np.load(label_path, allow_pickle=True)\n",
    "img_paths, img_ids, captions, label_strs, train_ids, val_ids, test_ids, label_names = out\n",
    "    \n",
    "# check correctness of lens\n",
    "assert len(img_paths) == len(img_ids)\n",
    "assert len(img_paths) == len(train_ids) + len(val_ids) + len(test_ids)\n",
    "assert len(captions) % len(img_ids) == 0\n",
    "assert len(img_ids) == len(label_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c6628-6063-403c-a305-193ccd2f85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(sns.boxplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3dcd74-4cf5-45e4-93fc-d4d5b874412f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_per_img = np.array([len(labels) for labels in label_strs])\n",
    "#sns.boxplot(x=labels_per_img)\n",
    "sns.violinplot(x=labels_per_img)\n",
    "\n",
    "p = plt.title(\"Violinplot of the number of labels per image\")\n",
    "plt.xlabel(\"Number of labels per image\")\n",
    "plt.ylabel(\"Density\")\n",
    "print(\"Num images without label: \", len(labels_per_img[labels_per_img == 0]))\n",
    "print(\"Median: \", np.median(labels_per_img))\n",
    "\n",
    "plt.savefig(os.path.join(dataset, f\"labels_per_img.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5423d4a2-ae78-46c2-9d45-81ace24b48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "imgs_per_label = defaultdict(int)\n",
    "for img_labels in label_strs:\n",
    "    for l in set(img_labels):\n",
    "        imgs_per_label[l] += 1\n",
    "imgs_per_label_df = pd.Series(list(imgs_per_label.values()), index=list(imgs_per_label.keys()))\n",
    "imgs_per_label_df = imgs_per_label_df.sort_values(ascending=False)\n",
    "\n",
    "show_df = pd.concat([imgs_per_label_df.iloc[:20], pd.Series([102, 101, 100], index=[\"...\", \" ...\", \"  ...\"]), imgs_per_label_df.iloc[-20:]], axis=0)\n",
    "#show_df = np.ln(show_df)\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.barplot(x=show_df, y= show_df.index, palette=\"summer\")\n",
    "plt.title(\"Top 20 most frequent and least frequent labels\")\n",
    "plt.xlabel(\"Images per label\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.savefig(os.path.join(dataset, f\"imgs_per_label.pdf\"), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6c0feb-f318-4b6b-ad70-295d72c0ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_per_img = len(captions) // len(img_paths)\n",
    "captions_per_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4aeb34a-3990-4219-8497-da6a4b001997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show qualitative data\n",
    "idx = 3\n",
    "caption_idx = idx * captions_per_img\n",
    "\n",
    "img = Image.open(img_paths[idx])\n",
    "fig = plt.figure()\n",
    "p = plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Sample image\", fontweight=\"bold\")\n",
    "plt.text(0, 400, r\"$\\bf{Caption: }$\" + captions[\"caption_text\"].iloc[caption_idx].strip(\".\").strip())\n",
    "plt.text(0, 450, r\"$\\bf{Labels: }$\" + \", \".join(label_strs[idx]))\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(dataset, f\"sample_{idx}.pdf\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0896801-de04-45b8-9f63-d3d65f56342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(img_idx, return_img=False):\n",
    "    img_name = img_ids[img_idx]\n",
    "    img_captions = [captions.iloc[img_idx * 5 + i][\"caption_text\"] for i in range(5)]\n",
    "    img = Image.open(img_paths[img_idx]) if return_img else img_paths[img_idx]\n",
    "    return img_name, img_captions, img\n",
    "\n",
    "test = False\n",
    "if test:\n",
    "    img_idx = 666\n",
    "\n",
    "    img_name, img_captions, img = get_data(img_idx, return_img=True)\n",
    "    print(img_name)\n",
    "    print(img_captions)\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df097e-3fd5-428a-b297-391f5e0535ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "print(clip.available_models())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_name = \"RN50\"\n",
    "model, transform = clip.load(clip_name, jit=False, device=device)\n",
    "load_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae723b-678a-4603-8999-2c2be0e542ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_image_features, get_text_features\n",
    "\n",
    "test_sims = False\n",
    "idx = 5002\n",
    "personal_caption = \"\"\n",
    "\n",
    "if test_sims:\n",
    "    img_name, img_captions, img = get_data(idx)\n",
    "    if len(personal_caption) > 0:\n",
    "        img_captions.append(personal_caption)\n",
    "\n",
    "\n",
    "    img_features = get_image_features([img], model, transform, device, \n",
    "                                      load_path, batch_size=16, save=False)\n",
    "    caption_features = get_text_features(img_captions, model, device, \n",
    "                                         load_path, batch_size=16, save=False)\n",
    "\n",
    "    sims = torch.cosine_similarity(img_features, caption_features)\n",
    "    top_k = sims.topk(k=5)\n",
    "    indices = top_k.indices\n",
    "    vals = top_k.values\n",
    "\n",
    "    Image.open(img).show()\n",
    "    for idx, val in zip(indices, vals):\n",
    "        print(val, img_captions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5e5e87-5c28-429b-809e-8db50878b85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = os.path.join(dataset, \"feats\")\n",
    "clip_load_name = clip_name.replace(\"/\", \"_\")\n",
    "clip_load_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca60bca-369b-4299-918b-24bc99cecadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(load_path, exist_ok=True)\n",
    "bs = 32\n",
    "\n",
    "img_features = get_image_features(img_paths, model, transform, device, \n",
    "                                  os.path.join(load_path, f\"{clip_load_name}_img_feats.pt\"), \n",
    "                                  batch_size=bs, save=True)\n",
    "caption_features = get_text_features(captions[\"caption_text\"], model, device, \n",
    "                                     os.path.join(load_path, f\"{clip_load_name}_caption_feats.pt\"), \n",
    "                                     batch_size=bs, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae58ace-565e-4e10-8407-78f5a889035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, NoiseTunnel, Saliency\n",
    "import torchvision\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "def minmax(a):\n",
    "    min_ = a.min()\n",
    "    max_ = a.max()\n",
    "    return (a - min_) / (max_ - min_)\n",
    "\n",
    "\n",
    "class MeanModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model.encode_image(x).mean(dim=1).unsqueeze(-1)\n",
    "\n",
    "    \n",
    "class CosineModel(torch.nn.Module):\n",
    "    def __init__(self, model, tokenized_text):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.text_feats = model.encode_text(tokenized_text)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        #img, tokenized_text = inputs\n",
    "        img_feats = self.model.encode_image(img)\n",
    "        #print(img_feats.squeeze().shape)\n",
    "        #return img_feats.squeeze()\n",
    "        return torch.cosine_similarity(img_feats, self.text_feats).unsqueeze(-1)\n",
    "\n",
    "\n",
    "def get_saliency(model, img_path, transform):\n",
    "    # wrap model to get mean feature\n",
    "    model = MeanModelWrapper(model).cpu().float()\n",
    "    # load img\n",
    "    img = transform(Image.open(img_path)).unsqueeze(0).cpu().float()\n",
    "    img.requires_grad = True\n",
    "    # get attribution\n",
    "    ig = IntegratedGradients(model)\n",
    "    #ig = Saliency(model)\n",
    "    nt = NoiseTunnel(ig)\n",
    "    attribution = nt.attribute(img, nt_type='smoothgrad',\n",
    "                               nt_samples=10, target=0)\n",
    "    return attribution\n",
    "\n",
    "\n",
    "def get_saliency_cosine(model, img_path, text, transform, std=0.5, \n",
    "                        nt_samples=100, use_ig=False, nt_type=\"smoothgrad\",\n",
    "                        abs=False, pos=False, t=5, perc=1):\n",
    "    device = torch.device(\"cuda\")\n",
    "    dtype = torch.float\n",
    "    # tokenize text\n",
    "    tokenized = clip.tokenize(text, truncate=True).to(device)\n",
    "    # wrap model to get cosine sim of text and img. feed text in here to avoid casting to float when passing it in  nt.attribute\n",
    "    model.to(device).to(dtype)\n",
    "    model = CosineModel(model, tokenized)\n",
    "    # load img\n",
    "    img = transform(Image.open(img_path)).unsqueeze(0).to(device).to(dtype)\n",
    "    img.requires_grad = True\n",
    "    # get attribution\n",
    "    if use_ig:\n",
    "        ig = IntegratedGradients(model, multiply_by_inputs=True)\n",
    "        kwargs = {\"internal_batch_size\": 8, \"n_steps\": 50}\n",
    "    else:\n",
    "        ig = Saliency(model)\n",
    "        kwargs = {\"abs\": abs}\n",
    "    nt = NoiseTunnel(ig)\n",
    "    attribution = nt.attribute(img, nt_type=nt_type, nt_samples=nt_samples, \n",
    "                               target=0, stdevs=std, nt_samples_batch_size=4,\n",
    "                               **kwargs)\n",
    "    #attribution = attribution.squeeze().mean(dim=0)\n",
    "    # normed attribution\n",
    "    attribution = attribution.squeeze().mean(dim=0) # take mean over rgb dimension\n",
    "    \n",
    "    p = plt.hist(attribution.cpu().flatten().numpy(), bins=1000)\n",
    "    plt.show()\n",
    "    \n",
    "    if pos:\n",
    "        print(\"Min val: \", min(attribution[attribution > 0]))\n",
    "        attribution[attribution < 0] = min(attribution[attribution > 0])\n",
    "    \n",
    "    attribution = minmax(attribution)\n",
    "    #attribution = torch.softmax(attribution.flatten() * t, dim=0).reshape(*attribution.shape)\n",
    "    \n",
    "    quant = torch.quantile(attribution, 1 - perc / 100)\n",
    "    attribution[attribution > quant] = quant\n",
    "    quant = torch.quantile(attribution, perc / 100)\n",
    "    attribution[attribution < quant] = quant\n",
    "\n",
    "    \n",
    "    attribution = minmax(attribution)\n",
    "    \n",
    "    if not abs and not pos:\n",
    "        attribution = attribution * 2 - 1\n",
    "    return attribution            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650a104-5bc9-4839-aa59-0af246e693d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(img_path, text, std=0.0):\n",
    "    img = transform(Image.open(img_path)).unsqueeze(0).to(device).to(torch.float)\n",
    "    if std != 0:\n",
    "        img = img + torch.normal(0.0, std, list(img.shape), device=img.device)\n",
    "    img.requires_grad = True\n",
    "    img_feats = model.encode_image(img)\n",
    "\n",
    "    tokenized_text = clip.tokenize(text, truncate=True).to(device)\n",
    "    text_feats = model.encode_text(tokenized_text)\n",
    "\n",
    "    sim = torch.cosine_similarity(img_feats, text_feats).unsqueeze(-1)\n",
    "    sim.backward()\n",
    "\n",
    "    grad = img.grad\n",
    "    return grad\n",
    "\n",
    "def sg(img_path, text, std=1.0, nt=10, pos=False, absolute=False):\n",
    "    grads = torch.stack([get_grad(img_path, text, std=std) for _ in range(nt)])\n",
    "    if pos:\n",
    "        grads[grads < 0] = 0\n",
    "    if absolute:\n",
    "        grads = torch.abs(grads)\n",
    "    return torch.mean(grads, dim=0)\n",
    "    \n",
    "    \n",
    "\n",
    "#grad = get_grad(img_path, text, std=1.0)\n",
    "#mean_grad = sg(img_path, text, std=1.0, nt=10, pos=False, absolute=True)\n",
    "#p = plt.hist(mean_grad.cpu().flatten().numpy(), bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f120959-b31e-4e35-9c65-36527718443d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c67cf-61c6-4b63-a16a-a91a1cb37bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f2756-9822-4e5a-ad99-82fb3b8b10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "std = 1.0\n",
    "nt_samples = 30\n",
    "abs = True\n",
    "pos = False\n",
    "t = 1\n",
    "\n",
    "#img_path = img_paths[idx]\n",
    "img_path = \"ipod_note_apple.png\"\n",
    "#caption = captions[\"caption_text\"].iloc[idx * 5]\n",
    "#caption = \"wait a second, this is just an apple with a label saying ipod\"\n",
    "#caption = \"a backyard\"\n",
    "caption = \"an apple stem\"\n",
    "\n",
    "loaded_img = torchvision.transforms.Compose(transform.transforms[:-1])(Image.open(img_path)).squeeze()\n",
    "print(\"Caption:\", caption)\n",
    "#Image.open(img_path).show()\n",
    "to_pil(loaded_img).show()\n",
    "\n",
    "import cv2\n",
    "def show_cam_on_image(img, mask):\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "print(\"Custom SG:\")\n",
    "sal = sg(img_path, caption, std=std, nt=nt_samples, pos=pos, absolute=abs)\n",
    "sal = sal.squeeze().mean(dim=0)\n",
    "sal = (sal - sal.min()) / (sal.max() - sal.min())\n",
    "p = plt.hist(sal.cpu().flatten().numpy(), bins=1000)\n",
    "plt.show()\n",
    "cmap = None if abs or pos else \"coolwarm\"\n",
    "sns.heatmap(sal.squeeze().cpu().numpy(), cmap=cmap)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#cosine_sal = torch.abs(cosine_sal)\n",
    "to_pil(sal.squeeze()).show()\n",
    "to_pil(sal.squeeze() * loaded_img.squeeze().cuda()).show()\n",
    "# heatmap mask\n",
    "image = np.moveaxis(np.array(loaded_img), 0, -1)\n",
    "image_relevance = sal.cpu().unsqueeze(-1).float().numpy()\n",
    "vis = show_cam_on_image(image, image_relevance)\n",
    "vis = np.uint8(255 * vis)\n",
    "vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "plt.imshow(vis)\n",
    "plt.show()\n",
    "\n",
    "print(\"Smoothgrad\")\n",
    "cosine_sal = get_saliency_cosine(model, img_path, caption, transform, std=std, nt_samples=nt_samples, abs=abs, t=t, pos=pos)\n",
    "p = plt.hist(cosine_sal.cpu().flatten().numpy(), bins=1000)\n",
    "plt.show()\n",
    "cmap = None if abs or pos else \"coolwarm\"\n",
    "sns.heatmap(cosine_sal.squeeze().cpu().numpy(), cmap=cmap)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "cosine_sal[cosine_sal < 0.5] = 0\n",
    "cosine_sal = torch.abs(cosine_sal)\n",
    "to_pil(cosine_sal.squeeze()).show()\n",
    "to_pil(cosine_sal.squeeze() * loaded_img.squeeze().cuda()).show()\n",
    "\n",
    "# create heatmap from mask on image\n",
    "image = np.moveaxis(np.array(loaded_img), 0, -1)\n",
    "image_relevance = cosine_sal.cpu().unsqueeze(-1).float().numpy()\n",
    "vis = show_cam_on_image(image, image_relevance)\n",
    "vis = np.uint8(255 * vis)\n",
    "vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "plt.imshow(vis)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "vargrad = True\n",
    "if vargrad:\n",
    "    print(\"VarGrad\")\n",
    "    cosine_sal = get_saliency_cosine(model, img_path, caption, transform, std=std, nt_samples=nt_samples, nt_type=\"vargrad\", t=t, abs=abs)\n",
    "    p = plt.hist(minmax(cosine_sal).cpu().flatten().numpy(), bins=1000)\n",
    "    plt.show()\n",
    "    to_pil(minmax(cosine_sal).squeeze()).show()\n",
    "    to_pil(minmax(cosine_sal).squeeze() * loaded_img.squeeze().cuda()).show()\n",
    "ig = False\n",
    "if ig:\n",
    "    print(\"IG\")\n",
    "    cosine_sal = get_saliency_cosine(model, img_path, caption, transform, std=std, nt_samples=nt_samples // 5, use_ig=True)\n",
    "    cosine_sal = minmax(torch.abs(cosine_sal - 0.5))\n",
    "    p = plt.hist((cosine_sal).cpu().flatten().numpy(), bins=1000)\n",
    "    plt.show()\n",
    "    to_pil((cosine_sal).squeeze()).show()\n",
    "    to_pil((cosine_sal).squeeze() * loaded_img.squeeze().cuda()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d7e5f-0712-4511-9b4b-ced750fa089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2text\n",
    "img_idx = 20000\n",
    "\n",
    "sims = torch.cosine_similarity(img_features[img_idx].unsqueeze(0), caption_features)\n",
    "Image.open(img_paths[img_idx]).show()\n",
    "idx = sims.topk(k=1).indices[0].item()\n",
    "print(idx)\n",
    "get_data(idx // 5, return_img=True)[2].show()\n",
    "print(captions[\"caption_text\"].iloc[idx], \"\\n\")\n",
    "print(\"Real captions:\")\n",
    "for text in get_data(img_idx)[1]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d936543-ce05-42c8-abfc-04c79aed6423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text2img\n",
    "text_idx = 5004\n",
    "\n",
    "print(captions[\"caption_text\"].iloc[text_idx])\n",
    "sims = torch.cosine_similarity(caption_features[text_idx].unsqueeze(0), img_features)\n",
    "idx = sims.topk(k=1).indices[0].item()\n",
    "print(idx)\n",
    "print(\"Found:\")\n",
    "Image.open(img_paths[idx]).show()\n",
    "print(\"Ground truth: \")\n",
    "Image.open(img_paths[text_idx // 5]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e276c82-44b7-417f-85cb-68f8f01a1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_metrics(feats, caption_features):\n",
    "    sims = torch.cosine_similarity(feats, caption_features)\n",
    "    #pred_idcs: List[int] = sims.topk(k=10).indices.tolist()\n",
    "    pred_idcs = sims.topk(k=10).indices\n",
    "    return pred_idcs\n",
    "\n",
    "\n",
    "def calc_img2text_retrieval(img_features, caption_features, cap_per_img=5):\n",
    "    #calc_metrics = torch.jit.trace(calc_metrics, (img_features[0].unsqueeze(0), caption_features))\n",
    "    r1 = torch.zeros(img_features.shape[0])\n",
    "    r5 = torch.zeros(img_features.shape[0])\n",
    "    r10 = torch.zeros(img_features.shape[0])\n",
    "    for img_idx in tqdm(torch.arange(img_features.shape[0], device=img_features.device)):\n",
    "        feats = img_features[img_idx].unsqueeze(0)\n",
    "        #print(feats.device, caption_features.device, img_idx.device)\n",
    "        pred_idcs = calc_metrics(feats, caption_features)\n",
    "        ground_truth_idcs = torch.arange(cap_per_img, device=feats.device) + img_idx * cap_per_img\n",
    "        r1_val = any([pred_idcs[i] in ground_truth_idcs for i in range(1)])\n",
    "        r5_val = any([pred_idcs[i] in ground_truth_idcs for i in range(5)])\n",
    "        r10_val = any([pred_idcs[i] in ground_truth_idcs for i in range(10)])\n",
    "        r1[img_idx] = r1_val\n",
    "        r5[img_idx] = r5_val\n",
    "        r10[img_idx] = r10_val\n",
    "    return r1, r5, r10\n",
    "\n",
    "\n",
    "def calc_text2img_retrieval(caption_features, img_features, cap_per_img=5):\n",
    "    r1 = torch.zeros(caption_features.shape[0])\n",
    "    r5 = torch.zeros(caption_features.shape[0])\n",
    "    r10 = torch.zeros(caption_features.shape[0])\n",
    "    for cap_idx in tqdm(torch.arange(caption_features.shape[0], device=caption_features.device)):\n",
    "        feats = caption_features[cap_idx].unsqueeze(0)\n",
    "        #print(feats.device, caption_features.device, img_idx.device)\n",
    "        pred_idcs = calc_metrics(feats, img_features)\n",
    "        ground_truth_idx = cap_idx // cap_per_img\n",
    "        r1_val = (pred_idcs[:1] == ground_truth_idx).max()\n",
    "        r5_val = (pred_idcs[:5] == ground_truth_idx).max()\n",
    "        r10_val = (pred_idcs[:10] == ground_truth_idx).max()\n",
    "        r1[cap_idx] = r1_val\n",
    "        r5[cap_idx] = r5_val\n",
    "        r10[cap_idx] = r10_val\n",
    "    return r1, r5, r10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17156f5e-cc24-4113-b555-b2efabbd99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_retrieval_metrics = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448e001-00e4-483c-9638-7f38d953d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if calc_retrieval_metrics:\n",
    "    query_feats = img_features.cuda()\n",
    "    target_feats = caption_features.cuda()\n",
    "\n",
    "    now = time.time()\n",
    "    metrics = calc_img2text_retrieval(query_feats, target_feats)\n",
    "    print(time.time() - now)\n",
    "    for i in range(len(metrics)):\n",
    "        print(torch.mean(metrics[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963ee7f-9717-4a5d-b029-4f0c71e3954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_retrieval_metrics:\n",
    "\n",
    "    query_feats = caption_features[:].cuda()\n",
    "    target_feats = img_features.cuda()\n",
    "\n",
    "    now = time.time()\n",
    "    text2img_metrics = calc_text2img_retrieval(query_feats, target_feats)\n",
    "    print(time.time() - now)\n",
    "    for i in range(len(text2img_metrics)):\n",
    "        print(torch.mean(text2img_metrics[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a28a1d-ea5f-448f-b7ec-b6c0ee7e8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prompt(labels, prefix=\"\", suffix=\"\"):\n",
    "    new_labels = [prefix + l + suffix for l in labels]\n",
    "    return new_labels\n",
    "\n",
    "# zero shot prediction\n",
    "use_multi_label_setting = False\n",
    "\n",
    "def norm(a):\n",
    "    return a / a.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "def create_label_encs(prefixes, use_multi_label_setting=False, use_norm=False):\n",
    "    if use_multi_label_setting:\n",
    "        # create queries\n",
    "        label_queries = [add_prompt(label_names, prefix=prefix) for prefix in prefixes]\n",
    "        # encode queries\n",
    "        label_encs = [get_text_features(label_query, model, device, \n",
    "                                             None,#os.path.join(load_path, f\"{clip_name}_caption_feats.pt\"), \n",
    "                                             batch_size=bs, save=False) for label_query in label_queries]\n",
    "        label_encs = torch.stack(label_encs).mean(dim=0)\n",
    "        print(label_encs.shape)\n",
    "    else:\n",
    "        # create baseline query, then positive query and construct negative encoding by \n",
    "        # subtracting pos encoding minus baseline encoding\n",
    "        all_diff_encs = []\n",
    "        for prefix in prefixes:\n",
    "            baseline_enc = get_text_features([prefix], model, device, None, batch_size=1, save=False)[0]\n",
    "            pos_encs = get_text_features(add_prompt(label_names, prefix=prefix), model, device, None, batch_size=32, save=False)\n",
    "            if use_norm:\n",
    "                diff_encs = pos_encs - baseline_enc#norm(norm(pos_encs) - norm(baseline_enc))\n",
    "            else:\n",
    "                diff_encs = pos_encs - baseline_enc\n",
    "            all_diff_encs.append(diff_encs)\n",
    "        diff_enc = norm(torch.stack(all_diff_encs)).mean(dim=0)\n",
    "        if use_norm:\n",
    "            neg_encs = norm(baseline_enc - diff_enc)#norm(norm(baseline_enc) - norm(diff_enc))\n",
    "        else:\n",
    "            neg_encs = baseline_enc - diff_enc\n",
    "\n",
    "        #label_encs = torch.stack(neg_encs)\n",
    "        label_encs = [pos_encs, neg_encs]\n",
    "    return label_encs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca81fcc-ad26-4978-83bd-d834f94cdf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_strs_to_int(label_strs, label_names):\n",
    "    return torch.tensor([l in label_strs for l in label_names]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a093098-277a-48e4-b522-ba961f1b03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracies(img_features, label_encs):\n",
    "    r1_sum = 0\n",
    "    r5_sum = 0\n",
    "    r10_sum = 0\n",
    "\n",
    "    for img_idx in tqdm(range(len(img_paths))):\n",
    "        img_enc = img_features[img_idx].unsqueeze(0)    \n",
    "        sims = torch.cosine_similarity(img_enc, label_encs)\n",
    "        pred_idcs = sims.topk(k=10).indices\n",
    "        top_idx = pred_idcs[0]\n",
    "        gt_labels = label_strs[img_idx]\n",
    "        gt_label_idcs = torch.where(label_strs_to_int(gt_labels, label_names))[0]\n",
    "        \n",
    "        r1 = 0\n",
    "        r5 = 0\n",
    "        r10 = 0\n",
    "        for i, pred in enumerate(pred_idcs):\n",
    "            if pred in gt_label_idcs:\n",
    "                if i == 0:\n",
    "                    r1, r5, r10 = 1, 1, 1\n",
    "                elif i < 5:\n",
    "                    r5, r10 = 1, 1\n",
    "                else:\n",
    "                    r10 = 1\n",
    "                break\n",
    "        r1_sum += r1\n",
    "        r5_sum += r5\n",
    "        r10_sum += r10\n",
    "    return (r1_sum / (img_idx + 1), r5_sum / (img_idx + 1), r10_sum / (img_idx + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f8892-7c8a-4a2a-91e3-56b6cd637521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570888f7-2fe3-44e5-bb73-af7bac6f741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_binary_acc(img_features, pos_label_encs, neg_label_encs):\n",
    "    # make list if not list yet\n",
    "    if not isinstance(pos_label_encs, list):\n",
    "        pos_label_encs = [pos_label_encs]\n",
    "        neg_label_encs = [neg_label_encs]\n",
    "    \n",
    "    accs = []\n",
    "    for img_idx in tqdm(range(len(img_paths))):\n",
    "        gt_labels = label_strs[img_idx]\n",
    "        gt_label_idcs = torch.where(label_strs_to_int(gt_labels, label_names))[0]\n",
    "        gt_binary = torch.zeros(len(label_names)).bool()\n",
    "        gt_binary[gt_label_idcs] = True\n",
    "        \n",
    "        img_enc = img_features[img_idx].unsqueeze(0)    \n",
    "        \n",
    "        img_pos_sims = []\n",
    "        img_neg_sims = []\n",
    "        for pos_encs, neg_encs in zip(pos_label_encs, neg_label_encs):\n",
    "            pos_sims = torch.cosine_similarity(img_enc, pos_encs)\n",
    "            neg_sims = torch.cosine_similarity(img_enc, neg_encs)\n",
    "            img_pos_sims.append(pos_sims)\n",
    "            img_neg_sims.append(neg_sims)\n",
    "            \n",
    "            \n",
    "        pos_sims = torch.stack(img_pos_sims).mean(dim=0)\n",
    "        neg_sims = torch.stack(img_neg_sims).mean(dim=0)\n",
    "\n",
    "        binary_preds = pos_sims > neg_sims\n",
    "        #binary_preds = torch.zeros(len(label_names))\n",
    "        preds = torch.where(binary_preds)[0]\n",
    "        acc = (binary_preds == gt_binary).float().mean()\n",
    "        #img_accs.append(acc)\n",
    "        #acc = torch.mean(torch.stack(img_accs))\n",
    "        #print(gt_label_idcs)\n",
    "        #print(len(preds))\n",
    "        #print(acc)\n",
    "        #print()\n",
    "        accs.append(acc)\n",
    "\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a60332-2448-4c83-ba5d-9cd55e9f5be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"A picture of \"]\n",
    "#prefixes = [ \"stock image \"]\n",
    "accs = calc_binary_acc(img_features, \n",
    "                       *create_label_encs(prefixes, use_multi_label_setting=False, use_norm=False))\n",
    "torch.mean(torch.stack(accs)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62a81f-dfe8-4633-943f-66ea441a943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"A photo of \"]\n",
    "#prefixes = [ \"stock image \"]\n",
    "accs = calc_binary_acc(img_features, \n",
    "                       *create_label_encs(prefixes, use_multi_label_setting=False, use_norm=False))\n",
    "torch.mean(torch.stack(accs)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb9028-2bfc-4ffe-b303-35250838d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"A photo of a people. \"]\n",
    "#prefixes = [ \"stock image \"]\n",
    "accs = calc_binary_acc(img_features, \n",
    "                       *create_label_encs(prefixes, use_multi_label_setting=False, use_norm=True))\n",
    "torch.mean(torch.stack(accs)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a7a68-9fef-4936-839b-7d8c6f25be6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c85ef-0edf-4272-9bf0-83fd1a82b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"A photo of \", \"A \", \"A picture of \", \"picture \", \"A picture of a person. \", \"flickr. \", \"stock image. \"]\n",
    "\n",
    "pos_encs = []\n",
    "neg_encs = []\n",
    "\n",
    "for prefix in prefixes:\n",
    "    pos_enc, neg_enc = create_label_encs([prefix], use_multi_label_setting=False, use_norm=False)\n",
    "    pos_encs.append(pos_enc)\n",
    "    neg_encs.append(neg_enc)\n",
    "\n",
    "accs = calc_binary_acc(img_features, pos_encs, neg_encs)\n",
    "torch.mean(torch.stack(accs)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b0cc4d-1232-49f6-bca6-09ec7021ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string    \n",
    "import random # define the random module  \n",
    "S = 10  # number of characters in the string.  \n",
    "# call random.choices() string module to find the string in Uppercase + numeric data.  \n",
    "prefixes = [''.join(random.choices(string.ascii_uppercase + string.digits, k = S)) + \". \" for _ in range(5)]\n",
    "\n",
    "pos_encs = []\n",
    "neg_encs = []\n",
    "\n",
    "for prefix in prefixes:\n",
    "    pos_enc, neg_enc = create_label_encs([prefix], use_multi_label_setting=False, use_norm=False)\n",
    "    pos_encs.append(pos_enc)\n",
    "    neg_encs.append(neg_enc)\n",
    "\n",
    "accs = calc_binary_acc(img_features, pos_encs, neg_encs)\n",
    "torch.mean(torch.stack(accs)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598564f5-4693-474b-a025-eedcffb8de37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda6cd7f-a507-4934-983d-be4d0f7c2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"A photo of \", \"a \", \"A painting of \", \"flickr. \", \"Artstation \", \"stock image \"]\n",
    "#prefixes = [ \"stock image \"]\n",
    "accs = calc_binary_acc(img_features, *create_label_encs(prefixes,\n",
    "                                                        use_multi_label_setting=False, use_norm=True))\n",
    "torch.mean(torch.stack(accs)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118cf772-82cd-4763-a545-18312474ae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixes = [\"A photo of \", \"a \", \"A painting of \", \"flickr. \", \"Artstation \", \"stock image \"]\n",
    "#prefixes = [ \"stock image \"]\n",
    "accs = calc_binary_acc(img_features, *create_label_encs(prefixes,\n",
    "                                                        use_multi_label_setting=False))\n",
    "torch.mean(torch.stack(accs)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d58dad1-1863-4e32-8008-5c4b1405334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_accuracies(img_features, create_label_encs(prefixes, use_multi_label_setting=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37793daf-0a1e-46de-b295-f83e5d0184a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fa85c0-aa12-4f5c-b197-98815d6995ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encs, neg_encs = create_label_encs([\"A photo of \"], use_multi_label_setting=False)\n",
    "baseline_enc = get_text_features([\"A photo of \"], model, device, None, batch_size=1, save=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b47c72c-30ab-49da-a80f-35f9990eb37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encs = create_label_encs([\"A photo with \"], use_multi_label_setting=1)\n",
    "neg_encs = create_label_encs([\"A photo without \"], use_multi_label_setting=1)\n",
    "accs = calc_binary_acc(img_features, pos_encs, neg_encs)\n",
    "print(torch.stack(accs).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119be24-a1f9-49fb-abb9-8315fb6fb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encs = create_label_encs([\"A \"], use_multi_label_setting=1)\n",
    "neg_encs = create_label_encs([\"Not a \"], use_multi_label_setting=1)\n",
    "accs = calc_binary_acc(img_features, pos_encs, neg_encs)\n",
    "print(torch.stack(accs).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d2d36-2ccd-407f-a99e-9aba9f1a0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 1 == 2:\n",
    "    accs = []\n",
    "    for img_idx in tqdm(range(len(img_paths))[:5]):\n",
    "        gt_labels = label_strs[img_idx]\n",
    "        gt_label_idcs = torch.where(label_strs_to_int(gt_labels, label_names))[0]\n",
    "        gt_binary = torch.zeros(len(label_names)).bool()\n",
    "        gt_binary[gt_label_idcs] = True\n",
    "\n",
    "        img_enc = img_features[img_idx].unsqueeze(0)    \n",
    "\n",
    "        pos_sims = torch.cosine_similarity(img_enc, pos_encs)\n",
    "        neg_sims = torch.cosine_similarity(img_enc, neg_encs)\n",
    "        base_sims = torch.cosine_similarity(img_enc, baseline_enc.unsqueeze(0))\n",
    "\n",
    "        easy_pos_sims = torch.cosine_similarity(img_enc, easy_pos_enc)\n",
    "        easy_neg_sims = torch.cosine_similarity(img_enc, easy_neg_enc)\n",
    "        easy_preds = easy_pos_sims > easy_neg_sims\n",
    "        acc = (easy_preds == gt_binary).float().mean()\n",
    "        print(np.array(label_names)[gt_label_idcs])\n",
    "        print(np.array(label_names)[easy_preds])\n",
    "        print()\n",
    "        accs.append(acc.item())\n",
    "        #print(gt_label_idcs)\n",
    "        #print(pos_sims[:10])\n",
    "        #print(neg_sims[:10])\n",
    "        #print(baseline_enc[:10])\n",
    "        #print()\n",
    "        #print(easy_pos_sims[:10])\n",
    "        #print(easy_neg_sims[:10])\n",
    "    print(np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8a02a-5917-40da-a09e-ccdee6b09d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc006dfa-8e48-4ba4-8529-ad012f86f23b",
   "metadata": {},
   "source": [
    "# Feature-based classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a1de45-aa2a-41b9-b131-70ff6cc94116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cee82f6-54be-4e11-a211-af443fe540dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {label_names[i]: i for i in range(len(label_names))}  # assigns idx to each label\n",
    "dense_label_arr = np.zeros((len(label_strs), len(label_names)))\n",
    "for row, labels in zip(dense_label_arr, label_strs):\n",
    "    label_idcs = [label_dict[label] for label in labels]\n",
    "    row[label_idcs] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d052b51d-f535-40e5-9eef-b1a0826a4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partition(ids):\n",
    "    mask = np.zeros(len(img_ids))\n",
    "    \n",
    "    \n",
    "    \n",
    "    mask = [i for i in range(len(img_ids)) if img_ids[i] in list(ids)]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de7537-d2ba-474e-8844-07d5413f3c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = [i for i in range(len(img_ids)) if img_ids[i] in list(train_ids[0])]\n",
    "print(sum(train_mask))\n",
    "val_mask = [i for i in range(len(img_ids)) if img_ids[i] in list(val_ids[0])]\n",
    "print(sum(val_mask))\n",
    "test_mask = [i for i in range(len(img_ids)) if img_ids[i] in list(test_ids[0])]\n",
    "print(sum(test_mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd8783-e005-40f4-9c19-607f1354e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clip_feats = img_features[train_mask]\n",
    "train_labels = dense_label_arr[train_mask]\n",
    "val_clip_feats = img_features[val_mask]\n",
    "val_labels = dense_label_arr[val_mask]\n",
    "test_clip_feats = img_features[test_mask]\n",
    "test_labels = dense_label_arr[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497c892-f5bb-43c7-9505-f486a69c7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def multilabel_acc(classifier, train_feats, train_labels, test_feats, test_labels, v=0):\n",
    "    scaler = StandardScaler()\n",
    "    train_feats = scaler.fit_transform(train_feats)\n",
    "    test_feats = scaler.transform(test_feats)\n",
    "    \n",
    "    accs = []\n",
    "    for label_idx in tqdm(range(train_labels.shape[1])):\n",
    "        classifier.fit(train_feats, train_labels[:, label_idx])\n",
    "\n",
    "        test_preds = classifier.predict(test_feats)\n",
    "        test_acc = (test_preds == test_labels[:, label_idx]).astype(float).mean()\n",
    "        mean_label = test_labels[:, label_idx].mean()\n",
    "        if v:\n",
    "            print(test_acc, mean_label if mean_label > 0.5 else 1 - mean_label)\n",
    "        accs.append(test_acc)\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560ca5f-8206-4a7c-bff8-9802edfd4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier(tree_method=\"gpu_hist\", eval_metric=\"error\", n_jobs=1)\n",
    "xgb_accs = multilabel_acc(classifier, \n",
    "                      train_clip_feats.numpy(), train_labels, \n",
    "                      test_clip_feats.numpy(), test_labels)\n",
    "print(np.mean(xgb_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee862d-4a37-4ca6-b677-29681e01bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier = XGBClassifier(n_estimators=10,\n",
    "                           tree_method=\"gpu_hist\", eval_metric=\"error\", n_jobs=1)\n",
    "xgb_10_accs = multilabel_acc(classifier, \n",
    "                      train_clip_feats.numpy(), train_labels, \n",
    "                      test_clip_feats.numpy(), test_labels)\n",
    "print(np.mean(xgb_10_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51274c46-670e-4865-a33b-fcace7ffd5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(n_jobs=5, solver=\"lbfgs\")\n",
    "logreg_accs = multilabel_acc(classifier, \n",
    "                      train_clip_feats.numpy(), train_labels, \n",
    "                      test_clip_feats.numpy(), test_labels,\n",
    "                      v=0)\n",
    "print(np.mean(logreg_accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8e04b6-5c85-466e-a30c-dbe52fa8192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb - 10 est: 98.81879999999998\n",
    "# xgb - default: 0.9884359999999999\n",
    "# linear reg - default: 98.87259999999999\n",
    "# baseline: 98.6002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3d396-8106-48e7-9b7f-af09487c2520",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - test_labels.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52271794-b3e2-44dd-a36c-c6dc2dc7cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "baselines = []\n",
    "for l in test_labels:\n",
    "    mean = l.mean()\n",
    "    if mean < 0.5:\n",
    "        mean = 1 - mean\n",
    "    baselines.append(mean)\n",
    "np.mean(baselines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae70f4-6aa8-4da5-b4be-7d99dc4562b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a71311-84b1-4d4e-b183-927d8a6829bc",
   "metadata": {},
   "source": [
    "# Finetuning CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6e4c0-b908-4055-9f6b-e1c62c6d4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model, transform\n",
    "\n",
    "# 1. only fine-tune mlp on top of frozen-features\n",
    "# 2. (1.) + fine-tune layer-norms\n",
    "\n",
    "#https://github.com/openai/CLIP/issues/57\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        if p.grad is not None:\n",
    "            p.grad.data = p.grad.data.float() \n",
    "\n",
    "\n",
    "class CLIPClassifier(torch.nn.Module):\n",
    "    def __init__(self, model, mode, num_labels):\n",
    "        super().__init__()\n",
    "        self.mode = mode\n",
    "        \n",
    "        # create layers\n",
    "        self.model = model\n",
    "        out_feat_size = model.text_projection.shape[1]\n",
    "        self.out = torch.nn.Linear(out_feat_size, num_labels)\n",
    "        \n",
    "        if mode == \"freeze\":\n",
    "            # delete layers as we now just feed in the precomputed features\n",
    "            #self.model = model.clone()\n",
    "            #self.model.encode_image = torch.nn.Identity()\n",
    "            #self.model.encode_text = torch.nn.Identity()\n",
    "            for p in self.model.parameters():\n",
    "                p.requires_grad = False\n",
    "        elif mode == \"train_norm\":\n",
    "            for n, p in self.model.named_parameters():\n",
    "                p.requires_grad = \".ln_\" in n or \".bn_\" in n\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode_image(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "    def encode_image(self, x):\n",
    "        if mode == \"freeze\":\n",
    "            return x\n",
    "        else:\n",
    "            return self.model.encode_image(x)\n",
    "    \n",
    "    def encode_text(self, x):\n",
    "        if mode == \"freeze\":\n",
    "            return x\n",
    "        else:\n",
    "            return self.model.encode_text(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b3c79a-fddb-4e06-b087-5b0a42a740b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "mode = \"freeze\"\n",
    "\n",
    "num_labels = train_labels.shape[1]\n",
    "clip_classifier = CLIPClassifier(model, mode, num_labels)\n",
    "\n",
    "if mode == \"freeze\":\n",
    "    train_ds = TensorDataset(train_clip_feats, \n",
    "                             torch.from_numpy(train_labels))\n",
    "    val_ds = TensorDataset(val_clip_feats, \n",
    "                           torch.from_numpy(val_labels))\n",
    "    test_ds = TensorDataset(test_clip_feats, \n",
    "                                  torch.from_numpy(test_labels))\n",
    "elif mode == \"train_norm\":\n",
    "    # create image datasets\n",
    "    pass\n",
    "\n",
    "bs = 32\n",
    "train_dl = DataLoader(train_ds, num_workers=0, shuffle=True, batch_size=bs)\n",
    "val_dl = DataLoader(val_ds, num_workers=0, shuffle=False, batch_size=bs * 4)\n",
    "test_dl = DataLoader(test_ds, num_workers=0, shuffle=False, batch_size=bs * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59680715-95a8-44c2-95d7-be8ebd3a6992",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(clip_classifier.parameters(),\n",
    "                             lr=5e-5,\n",
    "                             betas=(0.9, 0.98),\n",
    "                             eps=1e-6,\n",
    "                             weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for x, y in val_dl:\n",
    "        # calc loss\n",
    "        preds = clip_classifier(x)\n",
    "        loss = loss_img(preds, y)\n",
    "        # log\n",
    "        val_losses.append(loss)\n",
    "\n",
    "\n",
    "for epoch in range(20):\n",
    "    for x, y in tqdm(train_dl):\n",
    "        # calc classifier loss\n",
    "        preds = clip_classifier(x)\n",
    "        loss = loss_func(preds, y)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else : \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)\n",
    "        # log\n",
    "        train_losses.append(loss.detach())\n",
    "        \n",
    "    with torch.inference_mode():\n",
    "        for x, y in val_dl:\n",
    "            # calc loss\n",
    "            preds = clip_classifier(x)\n",
    "            loss = loss_img(preds, y)\n",
    "            # log\n",
    "            val_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c608bec-b9b5-4a4d-8a0d-489b3b585286",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = [e.item() for e in train_losses]\n",
    "val_losses = [e.item() for e in val_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2705079a-9949-424f-8615-49851e68744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = len(val_losses) // 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f21f85-6fb7-4606-85ca-dec3fa05f6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5cf5ea-595a-47ed-b17e-0d3fe78d93b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_per_epoch = [np.mean(val_losses[i * num_batches:i * num_batches + num_batches]) for i in range(6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62ca28-f22b-4635-bdef-8d56ac6bbec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e04a67-3711-48c2-9f61-2a4e5cecf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_losses[10::8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50caeaa-c474-4e20-8cda-42e62027d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567d263b-c505-46d2-ac2e-025462948c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0842063-c76e-4a0b-9078-7d1dd4567d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0732ac-a832-4ee3-bb24-cd1da1f5497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_y = []\n",
    "with torch.inference_mode():\n",
    "    for x, y in test_dl:\n",
    "        # calc loss\n",
    "        preds = clip_classifier(x)\n",
    "        all_preds.append(preds)\n",
    "        all_y.append(y)\n",
    "all_preds = torch.sigmoid(torch.cat(all_preds, dim=0)).numpy()\n",
    "all_y = torch.cat(all_y, dim=0).long().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a11cd4-07d3-4b8b-835d-cdc5d3a78c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6710131a-7dcc-4f37-aec6-a98d731cb953",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed5ca1a-ef9a-4b3b-b736-0bb6bb241ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "plt.plot(fprs, tprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35b9eb-dde9-4c4a-b5fe-3da59d2f106e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_thresholds = []\n",
    "\n",
    "for idx in range(all_y.shape[1]):\n",
    "    fprs, tprs, thresholds = sklearn.metrics.roc_curve(all_y[:, idx], all_preds[:, idx])\n",
    "\n",
    "    best_thresh_idx = np.argmax(tprs - fprs)\n",
    "    best_thresh = thresholds[best_thresh_idx]\n",
    "    best_thresholds.append(best_thresh)\n",
    "best_thresholds = np.array(best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263cf2a-0aa3-4642-ad5c-ab5c4f1fdcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_binary = np.array([all_preds[:, idx] > best_thresholds[idx] for idx in range(all_y.shape[1])]).reshape(*all_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc0dda0-0583-4b34-8477-0e60ac2f1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = np.array([(all_y[:, idx] & all_preds_binary[:, idx]).mean() \n",
    "                       for idx in range(all_y.shape[1])])\n",
    "accuracies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdccd136-676a-48e8-b3a0-181bb25b5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.hist(best_thresholds, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3cd6ac-5659-431e-b4f5-cb2543bd538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plt.hist(all_preds[:, idx], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d05bbe-cc8c-4aeb-a8d8-518b2e76bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresholds, tprs)\n",
    "plt.plot(thresholds, 1 - fprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9fd058-63b9-40f9-b31e-f57dba0bbe35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e679100d-8181-4f69-8459-5987ee9b559f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62aea1-5838-4de4-89a4-01ec0cb682b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = [sklearn.metrics.average_precision_score(all_y[:, label_idx], all_preds[:, label_idx], average='macro', pos_label=1)\n",
    "       for label_idx in range(all_y.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff825a46-8abb-4aa1-a219-0b650ab55fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "aps = [sklearn.metrics.average_precision_score(all_y[:, label_idx], all_preds[:, label_idx], average='macro', pos_label=1)\n",
    "       for label_idx in range(all_y.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f15415-f311-4d0d-83f2-3ebb77518a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(np.isnan(aps))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344536d6-53ce-4e2f-89cb-11bf8e14de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(aps[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436a7111-ea39-49a0-87d5-1c46b14f38cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(aps)[~np.isnan(aps)].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1ee703-e3d3-4931-b1cd-3f41b0bb9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4a26c-376d-4a81-9113-eac8a5a82d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3407b2a4-66f6-4bcc-9e77-349fa624385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc contrastive loss\n",
    "#list_image, list_txt = batch #list_images is list of image in numpy array(np.uint8), or list of PIL images\n",
    "#images= torch.stack([preprocess(Image.fromarray(img)) for img in list_image],dim=0).to(device) # omit the Image.fromarray if the images already in PIL format, change this line to images=list_image if using preprocess inside the dataset class\n",
    "#texts = clip.tokenize(list_txt).to(device)\n",
    "#logits_per_image, logits_per_text = model(images, texts)\n",
    "#ground_truth = torch.arange(BATCH_SIZE, dtype=torch.long, device=device)\n",
    "#loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
