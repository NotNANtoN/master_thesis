{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce0e6e-2f02-4693-b2e6-0e0c60fbeb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46aaac7-8aab-4fbd-bbb1-41f6d4f13f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"flickr30k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ecf981-718c-4692-a541-beee627490b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = {\"flickr30k\": \"/raid/datasets/f30k\",\n",
    "         \"mscoco\": \"/raid/datasets/coco\"}\n",
    "path = paths[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3297a-5280-4182-ab24-efea17e76c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "\n",
    "sub_folder = \"bua\"\n",
    "annots = os.listdir(os.path.join(path, \"features_36\", sub_folder))\n",
    "\n",
    "# test one\n",
    "loaded = np.load(os.path.join(path, \"features_36\", sub_folder, annots[0]), allow_pickle=True)\n",
    "print((loaded[\"info\"]).tolist()[\"objects_id\"])\n",
    "print((loaded[\"info\"]).tolist()[\"objects_conf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582882a-9d32-4aad-aa74-037df00259bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"/raid/datasets/coco/features_36/bua/\")\n",
    "\n",
    "files[0]\n",
    "\n",
    "loaded = np.load(os.path.join(path, \"features_36\", sub_folder, files[0]), allow_pickle=True)\n",
    "print(list(loaded.keys()))\n",
    "print(loaded[\"info\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a721d39-c50b-4d3b-9031-26e7a357a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def filter_stop_words(sent):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    word_tokens = word_tokenize(sent)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return \" \".join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5013a07-9474-413c-a3de-1cf6c2100ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_key_names(inv_dict, k=10, verbose=False, filter_words=None):\n",
    "    sorted_keys = sorted(inv_dict, key=lambda x: len(set([l // 5 for l in inv_dict[x]])), reverse=True)\n",
    "    valid_label_names = []\n",
    "    count = 0\n",
    "    for key in sorted_keys:\n",
    "        if filter_words is not None:\n",
    "            if key in filter_words:\n",
    "                continue\n",
    "        if verbose:\n",
    "            print(key)\n",
    "            print(len(inv_dict[key]))\n",
    "        if len(key) > 2:\n",
    "            valid_label_names.append(key)\n",
    "        count += 1\n",
    "        if count == k:\n",
    "            break\n",
    "    return valid_label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5e2cbf-9b9b-427c-ad0b-70ac2d7781fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# load img paths\n",
    "def is_img(img_name):\n",
    "    return img_name.endswith(\".jpg\") or img_name.endswith(\".jpeg\") or img_name.endswith(\".png\")\n",
    "\n",
    "\n",
    "def load_flickr(path, num_imgs=None):\n",
    "    img_folder = os.path.join(path, \"images\")\n",
    "    imgs = [i for i in os.listdir(img_folder) if is_img(i)]\n",
    "    imgs = sorted(imgs, key = lambda x: int(x.split(\".\")[0]))\n",
    "    img_ids = [int(x.split(\".\")[0]) for x in imgs]\n",
    "    img_paths = [os.path.join(img_folder, i) for i in imgs]\n",
    "\n",
    "    # load captions\n",
    "    captions = pd.read_csv(os.path.join(path, \"captions.txt\"), delimiter=\"|\")\n",
    "    captions[\"img_idx\"] = captions[\"image_name\"].apply(lambda x: int(x.split(\".\")[0]))\n",
    "    captions = captions.sort_values(\"img_idx\")\n",
    "    del captions[\"img_idx\"]\n",
    "    \n",
    "    if num_imgs is not None:\n",
    "        imgs = imgs[:num_imgs]\n",
    "        img_paths = imgs[:num_imgs]\n",
    "        img_ids = img_ids[:num_imgs]\n",
    "        captions = captions[:num_imgs * 5]\n",
    "    \n",
    "    # annotated bounding box labels:\n",
    "    # from https://github.com/BryanPlummer/flickr30k_entities\n",
    "    import sys\n",
    "    sys.path.append(\"/raid/datasets\")\n",
    "    from flickr30k_entities.flickr30k_entities_utils import get_annotations, get_sentence_data\n",
    "    from collections import defaultdict\n",
    "\n",
    "    labels = []\n",
    "    specific_dict = {}\n",
    "    broad_dict = {}\n",
    "    specific_inv_dict = defaultdict(list)\n",
    "    broad_inv_dict = defaultdict(list)\n",
    "\n",
    "    # load relations between img bounding boxes and captions\n",
    "    for img_id in tqdm(img_ids):\n",
    "        p = f\"/raid/datasets/flickr30k_entities/Annotations/{img_id}.xml\"\n",
    "        annot = get_annotations(p)\n",
    "        p = f\"/raid/datasets/flickr30k_entities/Sentences/{img_id}.txt\"\n",
    "        sent_data = get_sentence_data(p)\n",
    "\n",
    "        # get info from sentence data of which ids belong to image\n",
    "        for caption in sent_data:\n",
    "            phrases = caption[\"phrases\"]\n",
    "            for phrase in phrases:\n",
    "                id_ = int(phrase[\"phrase_id\"])\n",
    "\n",
    "                phrase_spec = phrase[\"phrase\"].lower()\n",
    "                phrase_spec = filter_stop_words(phrase_spec)\n",
    "                phrase_broad = phrase[\"phrase_type\"][0].lower()\n",
    "\n",
    "                broad_dict[id_] = phrase_broad\n",
    "                broad_inv_dict[phrase_broad].append(id_)\n",
    "                if len(phrase_spec) > 2:\n",
    "                    specific_dict[id_] = phrase_spec\n",
    "                    specific_inv_dict[phrase_spec].append(id_)\n",
    "        # get info from bounding box about which phrase ids appear in the image\n",
    "        bb_labels = [int(l) for l in list(annot[\"boxes\"].keys()) if int(l) in specific_dict]\n",
    "        scene_labels = [int(l) for l in annot[\"scene\"] if int(l) in broad_dict]\n",
    "        img_labels = list(set(bb_labels + scene_labels))\n",
    "        img_labels = [int(l) for l in img_labels]\n",
    "        labels.append(img_labels)\n",
    "\n",
    "    print(\"Total number of labels: \", len(specific_inv_dict))\n",
    "    num_spec_labels = 250\n",
    "    filter_words = [\"one\", \"something\", \"foreground\", \"background\"]\n",
    "    specific_label_names = get_valid_key_names(specific_inv_dict, k=num_spec_labels, filter_words=filter_words)\n",
    "    filter_words = [\"other\", \"scene\", \"notvisual\"]\n",
    "    broad_label_names = get_valid_key_names(broad_inv_dict, k=100, filter_words=filter_words)\n",
    "    all_label_names = specific_label_names + broad_label_names\n",
    "    print(\"Selected num of labels: \", len(all_label_names))\n",
    "    print(all_label_names)\n",
    "\n",
    "    figsize = (6, 10)\n",
    "    if num_spec_labels > 100:\n",
    "        figsize = (6, 20)\n",
    "    elif num_spec_labels > 200:\n",
    "        figsize = (6, 35)\n",
    "\n",
    "    occurences_specific = [len(specific_inv_dict[k]) for k in specific_label_names]\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.barplot(y=specific_label_names, x=occurences_specific)\n",
    "    plt.savefig(\"flickr30k/specific_occurences.png\")\n",
    "    plt.show()\n",
    "\n",
    "    occurences_broad = [len(broad_inv_dict[k]) for k in broad_label_names]\n",
    "    sns.barplot(y=broad_label_names, x=occurences_broad)\n",
    "    plt.savefig(\"flickr30k/broad_occurences.png\")\n",
    "    plt.show()\n",
    "\n",
    "    # calculate real labels\n",
    "    ext_dict = {**broad_dict, **specific_dict}\n",
    "    label_strs = []\n",
    "    for label_list in labels:\n",
    "        label_strs.append([ext_dict[l] for l in label_list if ext_dict[l] in all_label_names])\n",
    "\n",
    "    #label_strs = [get_labels(l, broad_dict, specific_dict) for l in labels]\n",
    "\n",
    "    # get training splits\n",
    "    test_ids = pd.read_csv(\"/raid/datasets/flickr30k_entities/test.txt\", sep=\"\\n\", header=None)\n",
    "    train_ids = pd.read_csv(\"/raid/datasets/flickr30k_entities/train.txt\", sep=\"\\n\", header=None)\n",
    "    val_ids = pd.read_csv(\"/raid/datasets/flickr30k_entities/val.txt\", sep=\"\\n\", header=None)\n",
    "    print(\"Num train samples: \", len(train_ids))\n",
    "    print(\"Num val samples: \", len(val_ids))\n",
    "    print(\"Num test samples: \", len(test_ids))\n",
    "    \n",
    "    #return img_paths, captions, labels, specific_inv_dict, broad_inv_dict, specific_dict, broad_dict, all_label_names\n",
    "    return img_paths, img_ids, captions, label_strs, train_ids, val_ids, test_ids, all_label_names\n",
    "\n",
    "\n",
    "def load_coco(path):\n",
    "    img_folder = os.path.join(path, \"images\")\n",
    "    imgs = [i for i in os.listdir(img_folder) if is_img(i)]\n",
    "    imgs = sorted(imgs, key = lambda x: int(x.split(\".\")[0].split(\"_\")[1]))\n",
    "    img_ids = [int(x.split(\".\")[0]) for x in imgs]\n",
    "    img_paths = [os.path.join(img_folder, i) for i in imgs]\n",
    "\n",
    "    # load captions\n",
    "    train_caps = json.load(open(\"annotations/captions_train2014.json\", \"r\"))\n",
    "    file_name = train_caps[\"images\"][0][\"file_name\"]\n",
    "    id_ = train_caps[\"images\"][0][\"id\"]\n",
    "    \n",
    "    # load captions\n",
    "    captions = pd.read_csv(os.path.join(path, \"captions.txt\"), delimiter=\"|\")\n",
    "    captions[\"img_idx\"] = captions[\"image_name\"].apply(lambda x: int(x.split(\".\")[0]))\n",
    "    captions = captions.sort_values(\"img_idx\")\n",
    "    del captions[\"img_idx\"]\n",
    "    return img_paths, captions\n",
    "\n",
    "\n",
    "    return img_paths, img_ids, captions, label_strs, train_ids, val_ids, test_ids, all_label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90a8c3b-c10b-44dd-afec-8d14256b9b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset specifics\n",
    "if dataset == \"flickr30k\":\n",
    "    label_path = \"flickr30k/labels.npy\"\n",
    "    load_func = load_flickr\n",
    "elif dataset == \"coco\":\n",
    "    label_path = \"coco/labels.npy\"\n",
    "    load_func = load_coco\n",
    "\n",
    "# load or create\n",
    "if not os.path.exists(label_path):\n",
    "    out = load_func(path)\n",
    "    np.save(label_path, out)\n",
    "else:\n",
    "    out = np.load(label_path, allow_pickle=True)\n",
    "img_paths, img_ids, captions, label_strs, train_ids, val_ids, test_ids, label_names = out\n",
    "    \n",
    "# check correctness of lens\n",
    "assert len(img_paths) == len(img_ids)\n",
    "assert len(img_paths) == len(train_ids) + len(val_ids) + len(test_ids)\n",
    "assert len(captions) % len(img_ids) == 0\n",
    "assert len(img_ids) == len(label_strs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76ead8-7953-4d4a-9ed8-edce7bb53a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcdfcd3-8c53-4425-9988-d8d35f06e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(l) for l in label_strs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40c7b9-d50b-4f59-a419-7d39ca65f6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d87388-370c-4919-aeaf-21f8c2e8e6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0896801-de04-45b8-9f63-d3d65f56342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(img_idx, return_img=False):\n",
    "    img_name = img_ids[img_idx]\n",
    "    img_captions = [captions.iloc[img_idx * 5 + i][\"caption_text\"] for i in range(5)]\n",
    "    img = Image.open(img_paths[img_idx]) if return_img else img_paths[img_idx]\n",
    "    return img_name, img_captions, img\n",
    "\n",
    "test = False\n",
    "if test:\n",
    "    img_idx = 666\n",
    "\n",
    "    img_name, img_captions, img = get_data(img_idx, return_img=True)\n",
    "    print(img_name)\n",
    "    print(img_captions)\n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52df097e-3fd5-428a-b297-391f5e0535ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "print(clip.available_models())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_name = \"ViT-B/16\"\n",
    "model, transform = clip.load(clip_name, jit=False, device=device)\n",
    "load_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae723b-678a-4603-8999-2c2be0e542ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_image_features, get_text_features\n",
    "\n",
    "test_sims = False\n",
    "idx = 5002\n",
    "personal_caption = \"\"\n",
    "\n",
    "\n",
    "if test_sims:\n",
    "    img_name, img_captions, img = get_data(idx)\n",
    "    if len(personal_caption) > 0:\n",
    "        img_captions.append(personal_caption)\n",
    "\n",
    "\n",
    "    img_features = get_image_features([img], model, transform, device, \n",
    "                                      load_path, batch_size=16, save=False)\n",
    "    caption_features = get_text_features(img_captions, model, device, \n",
    "                                         load_path, batch_size=16, save=False)\n",
    "\n",
    "    sims = torch.cosine_similarity(img_features, caption_features)\n",
    "    top_k = sims.topk(k=5)\n",
    "    indices = top_k.indices\n",
    "    vals = top_k.values\n",
    "\n",
    "    Image.open(img).show()\n",
    "    for idx, val in zip(indices, vals):\n",
    "        print(val, img_captions[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca60bca-369b-4299-918b-24bc99cecadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = os.path.join(dataset, \"feats\")\n",
    "os.makedirs(load_path, exist_ok=True)\n",
    "bs = 128\n",
    "\n",
    "img_features = get_image_features(img_paths, model, transform, device, \n",
    "                                  os.path.join(load_path, f\"{clip_name}_img_feats.pt\"), \n",
    "                                  batch_size=bs, save=True)\n",
    "caption_features = get_text_features(captions[\"caption_text\"], model, device, \n",
    "                                     os.path.join(load_path, f\"{clip_name}_caption_feats.pt\"), \n",
    "                                     batch_size=bs, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae58ace-565e-4e10-8407-78f5a889035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, NoiseTunnel, Saliency\n",
    "import torchvision\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "def minmax(a):\n",
    "    min_ = a.min()\n",
    "    max_ = a.max()\n",
    "    return (a - min_) / (max_ - min_)\n",
    "\n",
    "\n",
    "class MeanModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model.encode_image(x).mean(dim=1).unsqueeze(-1)\n",
    "\n",
    "    \n",
    "class CosineModel(torch.nn.Module):\n",
    "    def __init__(self, model, tokenized_text):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.text_feats = model.encode_text(tokenized_text)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        #img, tokenized_text = inputs\n",
    "        img_feats = self.model.encode_image(img)\n",
    "        #print(img_feats.squeeze().shape)\n",
    "        #return img_feats.squeeze()\n",
    "        return torch.cosine_similarity(img_feats, self.text_feats).unsqueeze(-1)\n",
    "\n",
    "\n",
    "def get_saliency(model, img_path, transform):\n",
    "    # wrap model to get mean feature\n",
    "    model = MeanModelWrapper(model).cpu().float()\n",
    "    # load img\n",
    "    img = transform(Image.open(img_path)).unsqueeze(0).cpu().float()\n",
    "    img.requires_grad = True\n",
    "    # get attribution\n",
    "    ig = IntegratedGradients(model)\n",
    "    #ig = Saliency(model)\n",
    "    nt = NoiseTunnel(ig)\n",
    "    attribution = nt.attribute(img, nt_type='smoothgrad',\n",
    "                               nt_samples=10, target=0)\n",
    "    return attribution\n",
    "\n",
    "\n",
    "def get_saliency_cosine(model, img_path, text, transform, std=0.5, \n",
    "                        nt_samples=100, use_ig=False, nt_type=\"smoothgrad\",\n",
    "                        abs=False, t=5, perc=1):\n",
    "    device = torch.device(\"cuda\")\n",
    "    dtype = torch.float\n",
    "    # tokenize text\n",
    "    tokenized = clip.tokenize(text, truncate=True).to(device)\n",
    "    # wrap model to get cosine sim of text and img. feed text in here to avoid casting to float when passing it in  nt.attribute\n",
    "    model.to(device).to(dtype)\n",
    "    model = CosineModel(model, tokenized)\n",
    "    # load img\n",
    "    img = transform(Image.open(img_path)).unsqueeze(0).to(device).to(dtype)\n",
    "    img.requires_grad = True\n",
    "    # get attribution\n",
    "    if use_ig:\n",
    "        ig = IntegratedGradients(model, multiply_by_inputs=True)\n",
    "        kwargs = {\"internal_batch_size\": 8, \"n_steps\": 50}\n",
    "    else:\n",
    "        ig = Saliency(model)\n",
    "        kwargs = {\"abs\": abs}\n",
    "    nt = NoiseTunnel(ig)\n",
    "    inputs = img#(img, tokenized)\n",
    "    attribution = nt.attribute(inputs, nt_type=nt_type, nt_samples=nt_samples, \n",
    "                               target=0, stdevs=std, nt_samples_batch_size=4,\n",
    "                               **kwargs)\n",
    "    #attribution = attribution.squeeze().mean(dim=0)\n",
    "    # normed attribution\n",
    "    attribution = attribution.squeeze().mean(dim=0)\n",
    "    \n",
    "    attribution = minmax(attribution)\n",
    "    attribution = torch.softmax(attribution.flatten() * t, dim=0).reshape(*attribution.shape)\n",
    "    \n",
    "    quant = torch.quantile(attribution, 1 - perc / 100)\n",
    "    attribution[attribution > quant] = quant\n",
    "    quant = torch.quantile(attribution, perc / 100)\n",
    "    attribution[attribution < quant] = quant\n",
    "\n",
    "    \n",
    "    attribution = minmax(attribution)\n",
    "    \n",
    "    if not abs:\n",
    "        attribution = attribution * 2 - 1\n",
    "    return attribution            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c67cf-61c6-4b63-a16a-a91a1cb37bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f2756-9822-4e5a-ad99-82fb3b8b10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "std = 1.0\n",
    "nt_samples = 30\n",
    "abs = True\n",
    "t = 1\n",
    "\n",
    "img_path = img_paths[idx]\n",
    "caption = captions[\"caption_text\"].iloc[idx * 5]\n",
    "caption = \"A photo of cards\"\n",
    "\n",
    "loaded_img = torchvision.transforms.Compose(transform.transforms[:-1])(Image.open(img_path)).squeeze()\n",
    "print(\"Caption:\", caption)\n",
    "#Image.open(img_path).show()\n",
    "to_pil(loaded_img).show()\n",
    "print(\"Smoothgrad\")\n",
    "cosine_sal = get_saliency_cosine(model, img_path, caption, transform, std=std, nt_samples=nt_samples, abs=abs, t=t)\n",
    "p = plt.hist(cosine_sal.cpu().flatten().numpy(), bins=1000)\n",
    "plt.show()\n",
    "cmap = None if abs else \"coolwarm\"\n",
    "sns.heatmap(cosine_sal.squeeze().cpu().numpy(), cmap=cmap)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "cosine_sal = torch.abs(cosine_sal)\n",
    "to_pil(cosine_sal.squeeze()).show()\n",
    "to_pil(cosine_sal.squeeze() * loaded_img.squeeze().cuda()).show()\n",
    "vargrad = True\n",
    "if vargrad:\n",
    "    print(\"VarGrad\")\n",
    "    cosine_sal = get_saliency_cosine(model, img_path, caption, transform, std=std, nt_samples=nt_samples, nt_type=\"vargrad\", t=t, abs=abs)\n",
    "    p = plt.hist(minmax(cosine_sal).cpu().flatten().numpy(), bins=1000)\n",
    "    plt.show()\n",
    "    to_pil(minmax(cosine_sal).squeeze()).show()\n",
    "    to_pil(minmax(cosine_sal).squeeze() * loaded_img.squeeze().cuda()).show()\n",
    "ig = False\n",
    "if ig:\n",
    "    print(\"IG\")\n",
    "    cosine_sal = get_saliency_cosine(model, img_path, caption, transform, std=std, nt_samples=nt_samples // 5, use_ig=True)\n",
    "    cosine_sal = minmax(torch.abs(cosine_sal - 0.5))\n",
    "    p = plt.hist((cosine_sal).cpu().flatten().numpy(), bins=1000)\n",
    "    plt.show()\n",
    "    to_pil((cosine_sal).squeeze()).show()\n",
    "    to_pil((cosine_sal).squeeze() * loaded_img.squeeze().cuda()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766d7e5f-0712-4511-9b4b-ced750fa089e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2text\n",
    "img_idx = 20000\n",
    "\n",
    "sims = torch.cosine_similarity(img_features[img_idx].unsqueeze(0), caption_features)\n",
    "Image.open(img_paths[img_idx]).show()\n",
    "idx = sims.topk(k=1).indices[0].item()\n",
    "print(idx)\n",
    "get_data(idx // 5, return_img=True)[2].show()\n",
    "print(captions[\"caption_text\"].iloc[idx], \"\\n\")\n",
    "print(\"Real captions:\")\n",
    "for text in get_data(img_idx)[1]:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d936543-ce05-42c8-abfc-04c79aed6423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text2img\n",
    "text_idx = 5004\n",
    "\n",
    "print(captions[\"caption_text\"].iloc[text_idx])\n",
    "sims = torch.cosine_similarity(caption_features[text_idx].unsqueeze(0), img_features)\n",
    "idx = sims.topk(k=1).indices[0].item()\n",
    "print(idx)\n",
    "print(\"Found:\")\n",
    "Image.open(img_paths[idx]).show()\n",
    "print(\"Ground truth: \")\n",
    "Image.open(img_paths[text_idx // 5]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e276c82-44b7-417f-85cb-68f8f01a1e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calc_metrics(feats, caption_features):\n",
    "    sims = torch.cosine_similarity(feats, caption_features)\n",
    "    #pred_idcs: List[int] = sims.topk(k=10).indices.tolist()\n",
    "    pred_idcs = sims.topk(k=10).indices\n",
    "    return pred_idcs\n",
    "\n",
    "\n",
    "def calc_img2text_retrieval(img_features, caption_features, cap_per_img=5):\n",
    "    #calc_metrics = torch.jit.trace(calc_metrics, (img_features[0].unsqueeze(0), caption_features))\n",
    "    r1 = torch.zeros(img_features.shape[0])\n",
    "    r5 = torch.zeros(img_features.shape[0])\n",
    "    r10 = torch.zeros(img_features.shape[0])\n",
    "    for img_idx in tqdm(torch.arange(img_features.shape[0], device=img_features.device)):\n",
    "        feats = img_features[img_idx].unsqueeze(0)\n",
    "        #print(feats.device, caption_features.device, img_idx.device)\n",
    "        pred_idcs = calc_metrics(feats, caption_features)\n",
    "        ground_truth_idcs = torch.arange(cap_per_img, device=feats.device) + img_idx * cap_per_img\n",
    "        r1_val = any([pred_idcs[i] in ground_truth_idcs for i in range(1)])\n",
    "        r5_val = any([pred_idcs[i] in ground_truth_idcs for i in range(5)])\n",
    "        r10_val = any([pred_idcs[i] in ground_truth_idcs for i in range(10)])\n",
    "        r1[img_idx] = r1_val\n",
    "        r5[img_idx] = r5_val\n",
    "        r10[img_idx] = r10_val\n",
    "    return r1, r5, r10\n",
    "\n",
    "\n",
    "def calc_text2img_retrieval(caption_features, img_features, cap_per_img=5):\n",
    "    r1 = torch.zeros(caption_features.shape[0])\n",
    "    r5 = torch.zeros(caption_features.shape[0])\n",
    "    r10 = torch.zeros(caption_features.shape[0])\n",
    "    for cap_idx in tqdm(torch.arange(caption_features.shape[0], device=caption_features.device)):\n",
    "        feats = caption_features[cap_idx].unsqueeze(0)\n",
    "        #print(feats.device, caption_features.device, img_idx.device)\n",
    "        pred_idcs = calc_metrics(feats, img_features)\n",
    "        ground_truth_idx = cap_idx // cap_per_img\n",
    "        r1_val = (pred_idcs[:1] == ground_truth_idx).max()\n",
    "        r5_val = (pred_idcs[:5] == ground_truth_idx).max()\n",
    "        r10_val = (pred_idcs[:10] == ground_truth_idx).max()\n",
    "        r1[cap_idx] = r1_val\n",
    "        r5[cap_idx] = r5_val\n",
    "        r10[cap_idx] = r10_val\n",
    "    return r1, r5, r10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17156f5e-cc24-4113-b555-b2efabbd99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats = img_features.cuda()\n",
    "target_feats = caption_features.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0448e001-00e4-483c-9638-7f38d953d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "now = time.time()\n",
    "metrics = calc_img2text_retrieval(query_feats, target_feats)\n",
    "print(time.time() - now)\n",
    "for i in range(len(metrics)):\n",
    "    print(torch.mean(metrics[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963ee7f-9717-4a5d-b029-4f0c71e3954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_feats = caption_features[:].cuda()\n",
    "target_feats = img_features.cuda()\n",
    "\n",
    "now = time.time()\n",
    "text2img_metrics = calc_text2img_retrieval(query_feats, target_feats)\n",
    "print(time.time() - now)\n",
    "for i in range(len(text2img_metrics)):\n",
    "    print(torch.mean(text2img_metrics[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a28a1d-ea5f-448f-b7ec-b6c0ee7e8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prompt(labels, prefix=\"\", suffix=\"\"):\n",
    "    new_labels = [prefix + l + suffix for l in labels]\n",
    "    return new_labels\n",
    "\n",
    "# zero shot prediction\n",
    "label_queries = [label_names]\n",
    "#label_queries.append(add_prompt(label_names, prefix=\"A photo of: \"))\n",
    "#label_queries.append(add_prompt(label_names, prefix=\"A photo of \"))\n",
    "#label_queries.append(add_prompt(label_names, prefix=\"A picture of \"))\n",
    "#label_queries.append(add_prompt(label_names, prefix=\"On this picture you can see: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46363ab0-e29b-490a-9f1a-99ebfb561d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encs = [get_text_features(label_query, model, device, \n",
    "                                     None,#os.path.join(load_path, f\"{clip_name}_caption_feats.pt\"), \n",
    "                                     batch_size=bs, save=False) for label_query in label_queries]\n",
    "label_enc = torch.stack(label_encs).mean(dim=0)\n",
    "label_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27930b42-1c49-4fad-86d7-ac383cf929b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f3a93-e4da-450e-85fc-44b4ec919e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca81fcc-ad26-4978-83bd-d834f94cdf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_strs_to_int(label_strs, label_names):\n",
    "    return torch.tensor([l in label_strs for l in label_names]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a093098-277a-48e4-b522-ba961f1b03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracies(img_features, label_encs):\n",
    "    r1_sum = 0\n",
    "    r5_sum = 0\n",
    "    r10_sum = 0\n",
    "\n",
    "    for img_idx in tqdm(range(len(img_paths))):\n",
    "        img_enc = img_features[img_idx].unsqueeze(0)    \n",
    "        sims = torch.cosine_similarity(img_enc, label_enc)\n",
    "        pred_idcs = sims.topk(k=10).indices\n",
    "        top_idx = pred_idcs[0]\n",
    "        gt_labels = label_strs[img_idx]\n",
    "        gt_label_idcs = torch.where(label_strs_to_int(gt_labels, label_names))[0]\n",
    "        \n",
    "        r1 = 0\n",
    "        r5 = 0\n",
    "        r10 = 0\n",
    "        for i, pred in enumerate(pred_idcs):\n",
    "            if pred in gt_label_idcs:\n",
    "                if i == 0:\n",
    "                    r1, r5, r10 = 1, 1, 1\n",
    "                elif i < 5:\n",
    "                    r5, r10 = 1, 1\n",
    "                else:\n",
    "                    r10 = 1\n",
    "                break\n",
    "        r1_sum += r1\n",
    "        r5_sum += r5\n",
    "        r10_sum += r10\n",
    "    return (r1_sum / (img_idx + 1), r5_sum / (img_idx + 1), r10_sum / (img_idx + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd38cf3-4182-410c-9d8a-e2fddbe1bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_accuracies(img_features, label_encs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0546ade2-9bfc-4fcb-993a-39a82a5ddcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_accuracies(img_features, label_encs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eced0937-b477-4bc8-a6af-b96f9fb8ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calc_accuracies(img_features, label_encs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118cf772-82cd-4763-a545-18312474ae7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
