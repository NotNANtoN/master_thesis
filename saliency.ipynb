{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd382c-9977-4912-afba-f19471e3b40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(img_path, text, std=0.0):\n",
    "    img = transform(Image.open(img_path)).unsqueeze(0).to(device).to(torch.float)\n",
    "    if std != 0:\n",
    "        img = img + torch.normal(0.0, std, list(img.shape), device=img.device)\n",
    "    img.requires_grad = True\n",
    "    img_feats = model.encode_image(img)\n",
    "\n",
    "    tokenized_text = clip.tokenize(text, truncate=True).to(device)\n",
    "    text_feats = model.encode_text(tokenized_text)\n",
    "\n",
    "    sim = torch.cosine_similarity(img_feats, text_feats).unsqueeze(-1)\n",
    "    sim.backward()\n",
    "\n",
    "    grad = img.grad\n",
    "    return grad\n",
    "\n",
    "def sg(img_path, text, std=1.0, nt=10, pos=False, absolute=False):\n",
    "    grads = torch.stack([get_grad(img_path, text, std=std) for _ in range(nt)])\n",
    "    if pos:\n",
    "        grads[grads < 0] = 0\n",
    "    if absolute:\n",
    "        grads = torch.abs(grads)\n",
    "    return torch.mean(grads, dim=0)\n",
    "    \n",
    "    \n",
    "\n",
    "#grad = get_grad(img_path, text, std=1.0)\n",
    "#mean_grad = sg(img_path, text, std=1.0, nt=10, pos=False, absolute=True)\n",
    "#p = plt.hist(mean_grad.cpu().flatten().numpy(), bins=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e75d8d-3a07-4277-a314-4dc7c6089d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import IntegratedGradients, NoiseTunnel, Saliency\n",
    "import torchvision\n",
    "\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "def minmax(a):\n",
    "    min_ = a.min()\n",
    "    max_ = a.max()\n",
    "    return (a - min_) / (max_ - min_)\n",
    "\n",
    "\n",
    "class MeanModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model.encode_image(x).mean(dim=1).unsqueeze(-1)\n",
    "\n",
    "    \n",
    "class CosineModel(torch.nn.Module):\n",
    "    def __init__(self, model, tokenized_text):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.text_feats = model.encode_text(tokenized_text)\n",
    "    \n",
    "    def forward(self, img):\n",
    "        #img, tokenized_text = inputs\n",
    "        img_feats = self.model.encode_image(img)\n",
    "        #print(img_feats.squeeze().shape)\n",
    "        #return img_feats.squeeze()\n",
    "        return torch.cosine_similarity(img_feats, self.text_feats).unsqueeze(-1)\n",
    "\n",
    "\n",
    "def get_saliency(model, img_path, transform):\n",
    "    # wrap model to get mean feature\n",
    "    model = MeanModelWrapper(model).cpu().float()\n",
    "    # load img\n",
    "    img = transform(Image.open(img_path)).unsqueeze(0).cpu().float()\n",
    "    img.requires_grad = True\n",
    "    # get attribution\n",
    "    ig = IntegratedGradients(model)\n",
    "    #ig = Saliency(model)\n",
    "    nt = NoiseTunnel(ig)\n",
    "    attribution = nt.attribute(img, nt_type='smoothgrad',\n",
    "                               nt_samples=10, target=0)\n",
    "    return attribution\n",
    "\n",
    "\n",
    "def get_saliency_cosine(model, img_path, text, transform, std=0.5, \n",
    "                        nt_samples=100, use_ig=False, nt_type=\"smoothgrad\",\n",
    "                        abs=False, pos=False, t=5, perc=1):\n",
    "    device = torch.device(\"cuda\")\n",
    "    dtype = torch.float\n",
    "    # tokenize text\n",
    "    tokenized = clip.tokenize(text, truncate=True).to(device)\n",
    "    # wrap model to get cosine sim of text and img. feed text in here to avoid casting to float when passing it in  nt.attribute\n",
    "    model.to(device).to(dtype)\n",
    "    model = CosineModel(model, tokenized)\n",
    "    # load img\n",
    "    img = transform(Image.open(img_path)).unsqueeze(0).to(device).to(dtype)\n",
    "    img.requires_grad = True\n",
    "    # get attribution\n",
    "    if use_ig:\n",
    "        ig = IntegratedGradients(model, multiply_by_inputs=True)\n",
    "        kwargs = {\"internal_batch_size\": 8, \"n_steps\": 50}\n",
    "    else:\n",
    "        ig = Saliency(model)\n",
    "        kwargs = {\"abs\": abs}\n",
    "    nt = NoiseTunnel(ig)\n",
    "    attribution = nt.attribute(img, nt_type=nt_type, nt_samples=nt_samples, \n",
    "                               target=0, stdevs=std, nt_samples_batch_size=4,\n",
    "                               **kwargs)\n",
    "    #attribution = attribution.squeeze().mean(dim=0)\n",
    "    # normed attribution\n",
    "    attribution = attribution.squeeze().mean(dim=0) # take mean over rgb dimension\n",
    "    \n",
    "    p = plt.hist(attribution.cpu().flatten().numpy(), bins=1000)\n",
    "    plt.show()\n",
    "    \n",
    "    if pos:\n",
    "        print(\"Min val: \", min(attribution[attribution > 0]))\n",
    "        attribution[attribution < 0] = min(attribution[attribution > 0])\n",
    "    \n",
    "    attribution = minmax(attribution)\n",
    "    #attribution = torch.softmax(attribution.flatten() * t, dim=0).reshape(*attribution.shape)\n",
    "    \n",
    "    quant = torch.quantile(attribution, 1 - perc / 100)\n",
    "    attribution[attribution > quant] = quant\n",
    "    quant = torch.quantile(attribution, perc / 100)\n",
    "    attribution[attribution < quant] = quant\n",
    "\n",
    "    \n",
    "    attribution = minmax(attribution)\n",
    "    \n",
    "    if not abs and not pos:\n",
    "        attribution = attribution * 2 - 1\n",
    "    return attribution            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d9cfb-effd-48d7-8fa4-1219274bbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "\n",
    "std = 1.0\n",
    "nt_samples = 30\n",
    "abs = True\n",
    "pos = False\n",
    "t = 1\n",
    "\n",
    "#img_path = img_paths[idx]\n",
    "img_path = \"ipod_note_apple.png\"\n",
    "#caption = captions[\"caption_text\"].iloc[idx * 5]\n",
    "#caption = \"wait a second, this is just an apple with a label saying ipod\"\n",
    "#caption = \"a backyard\"\n",
    "caption = \"an apple stem\"\n",
    "\n",
    "loaded_img = torchvision.transforms.Compose(transform.transforms[:-1])(Image.open(img_path)).squeeze()\n",
    "print(\"Caption:\", caption)\n",
    "#Image.open(img_path).show()\n",
    "to_pil(loaded_img).show()\n",
    "\n",
    "import cv2\n",
    "def show_cam_on_image(img, mask):\n",
    "    mask = (mask * 255).astype(np.uint8)\n",
    "    heatmap = cv2.applyColorMap(mask, cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    return cam\n",
    "\n",
    "print(\"Custom SG:\")\n",
    "sal = sg(img_path, caption, std=std, nt=nt_samples, pos=pos, absolute=abs)\n",
    "sal = sal.squeeze().mean(dim=0)\n",
    "sal = (sal - sal.min()) / (sal.max() - sal.min())\n",
    "p = plt.hist(sal.cpu().flatten().numpy(), bins=1000)\n",
    "plt.show()\n",
    "cmap = None if abs or pos else \"coolwarm\"\n",
    "sns.heatmap(sal.squeeze().cpu().numpy(), cmap=cmap)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#cosine_sal = torch.abs(cosine_sal)\n",
    "to_pil(sal.squeeze()).show()\n",
    "to_pil(sal.squeeze() * loaded_img.squeeze().cuda()).show()\n",
    "# heatmap mask\n",
    "image = np.moveaxis(np.array(loaded_img), 0, -1)\n",
    "image_relevance = sal.cpu().unsqueeze(-1).float().numpy()\n",
    "vis = show_cam_on_image(image, image_relevance)\n",
    "vis = np.uint8(255 * vis)\n",
    "vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "plt.imshow(vis)\n",
    "plt.show()\n",
    "\n",
    "print(\"Smoothgrad\")\n",
    "cosine_sal = get_saliency_cosine(model, img_path, caption, transform, std=std, nt_samples=nt_samples, abs=abs, t=t, pos=pos)\n",
    "p = plt.hist(cosine_sal.cpu().flatten().numpy(), bins=1000)\n",
    "plt.show()\n",
    "cmap = None if abs or pos else \"coolwarm\"\n",
    "sns.heatmap(cosine_sal.squeeze().cpu().numpy(), cmap=cmap)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "cosine_sal[cosine_sal < 0.5] = 0\n",
    "cosine_sal = torch.abs(cosine_sal)\n",
    "to_pil(cosine_sal.squeeze()).show()\n",
    "to_pil(cosine_sal.squeeze() * loaded_img.squeeze().cuda()).show()\n",
    "\n",
    "# create heatmap from mask on image\n",
    "image = np.moveaxis(np.array(loaded_img), 0, -1)\n",
    "image_relevance = cosine_sal.cpu().unsqueeze(-1).float().numpy()\n",
    "vis = show_cam_on_image(image, image_relevance)\n",
    "vis = np.uint8(255 * vis)\n",
    "vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "plt.imshow(vis)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "vargrad = True\n",
    "if vargrad:\n",
    "    print(\"VarGrad\")\n",
    "    cosine_sal = get_saliency_cosine(model, img_path, caption, transform, std=std, nt_samples=nt_samples, nt_type=\"vargrad\", t=t, abs=abs)\n",
    "    p = plt.hist(minmax(cosine_sal).cpu().flatten().numpy(), bins=1000)\n",
    "    plt.show()\n",
    "    to_pil(minmax(cosine_sal).squeeze()).show()\n",
    "    to_pil(minmax(cosine_sal).squeeze() * loaded_img.squeeze().cuda()).show()\n",
    "ig = False\n",
    "if ig:\n",
    "    print(\"IG\")\n",
    "    cosine_sal = get_saliency_cosine(model, img_path, caption, transform, std=std, nt_samples=nt_samples // 5, use_ig=True)\n",
    "    cosine_sal = minmax(torch.abs(cosine_sal - 0.5))\n",
    "    p = plt.hist((cosine_sal).cpu().flatten().numpy(), bins=1000)\n",
    "    plt.show()\n",
    "    to_pil((cosine_sal).squeeze()).show()\n",
    "    to_pil((cosine_sal).squeeze() * loaded_img.squeeze().cuda()).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
